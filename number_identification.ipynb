{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyOpSY2AP0jmUjSTJGZCqcow",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhi-213/Deep-learning/blob/main/number_identification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpGQfDFyaujD"
      },
      "outputs": [],
      "source": [
        "!pip list\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets,transforms\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "iNGE27_Ca5tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert image to tensor(images,height,width,color)\n",
        "transform = transforms.ToTensor()"
      ],
      "metadata": {
        "id": "mNoN9Rh5beMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train data\n",
        "train_data = datasets.MNIST(root='/cnn_data',train=True,download=True,transform=transform)"
      ],
      "metadata": {
        "id": "4Z-_WhMm0Jvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test data\n",
        "test_data = datasets.MNIST(root='/cnn_data',train=False,download=True,transform=transform)"
      ],
      "metadata": {
        "id": "8e7NlhDQ0pg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_data"
      ],
      "metadata": {
        "id": "v31rBW7k0vDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "id": "igWi8fYy03-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls\n"
      ],
      "metadata": {
        "id": "v69U6Qzi0_F9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create batch size\n",
        "train_loader = DataLoader(train_data,batch_size=10,shuffle=True)\n",
        "test_loader = DataLoader(test_data,batch_size=10,shuffle=False)"
      ],
      "metadata": {
        "id": "gWxQUb201IXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model classs\n",
        "class simplecnn(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(1,6,3,1)\n",
        "    self.conv2 = nn.Conv2d(6,16,3,1)\n",
        "    #full connected layer\n",
        "    self.fc1 = nn.Linear(5*5*16,120)\n",
        "    self.fc2 = nn.Linear(120,84)\n",
        "    self.fc3 = nn.Linear(84,10)\n",
        "\n",
        "  def forward(self,X):\n",
        "    X = F.relu(self.conv1(X))\n",
        "    X = F.max_pool2d(X,2,2)\n",
        "    X = F.relu(self.conv2(X))\n",
        "    X = F.max_pool2d(X,2,2)\n",
        "\n",
        "    X=X.view(-1,5*5*16)\n",
        "    X = F.relu(self.fc1(X))\n",
        "    X = F.relu(self.fc2(X))\n",
        "    X = self.fc3(X)\n",
        "    return F.log_softmax(X,dim=1)\n"
      ],
      "metadata": {
        "id": "v6NAsXmO2KmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#instence\n",
        "torch.manual_seed(41)\n",
        "model = simplecnn()\n",
        "model"
      ],
      "metadata": {
        "id": "evgnT6G33GgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)"
      ],
      "metadata": {
        "id": "gryT0mlU3Yz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "#create variable\n",
        "epochs = 5\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_correct = []\n",
        "test_correct = []\n",
        "\n",
        "#for loop\n",
        "for i in range(epochs):\n",
        "  trn_corr = 0\n",
        "  tst_corr = 0\n",
        "\n",
        "  #train\n",
        "  for b,(X_train,y_train) in enumerate(train_loader):\n",
        "    b+=1\n",
        "    y_pred = model(X_train)\n",
        "    loss = criterion(y_pred,y_train)\n",
        "    predicted = torch.max(y_pred.data,1)[1]\n",
        "    batch_corr = (predicted == y_train).sum()\n",
        "    trn_corr += batch_corr\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if b%100 == 0:\n",
        "      print(f'epoch:{i} Batch: {b} loss:{loss.item()}')\n",
        "\n",
        "  train_losses.append(loss)\n",
        "  train_correct.append(trn_corr)\n",
        "\n",
        "  #test\n",
        "  with torch.no_grad():\n",
        "    for b,(X_test,y_test) in enumerate(test_loader):\n",
        "      y_val = model(X_test)\n",
        "      predicted = torch.max(y_val.data,1)[1]\n",
        "      tst_corr += (predicted == y_test).sum()\n",
        "\n",
        "  loss = criterion(y_val,y_test)\n",
        "  test_losses.append(loss)\n",
        "  test_correct.append(tst_corr)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "current_time = time.time()\n",
        "print((current_time-start_time)/60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMEK7H1M3bAG",
        "outputId": "54b6228b-8326-4e71-a3eb-0ccb7af3bcb4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:0 Batch: 100 loss:0.9812231063842773\n",
            "epoch:0 Batch: 200 loss:0.43812650442123413\n",
            "epoch:0 Batch: 300 loss:0.28931525349617004\n",
            "epoch:0 Batch: 400 loss:0.08598048985004425\n",
            "epoch:0 Batch: 500 loss:0.33633095026016235\n",
            "epoch:0 Batch: 600 loss:0.1623610556125641\n",
            "epoch:0 Batch: 700 loss:0.5226176977157593\n",
            "epoch:0 Batch: 800 loss:0.6974474191665649\n",
            "epoch:0 Batch: 900 loss:0.20160897076129913\n",
            "epoch:0 Batch: 1000 loss:0.5349848866462708\n",
            "epoch:0 Batch: 1100 loss:0.8624889254570007\n",
            "epoch:0 Batch: 1200 loss:0.1502392590045929\n",
            "epoch:0 Batch: 1300 loss:0.025196203961968422\n",
            "epoch:0 Batch: 1400 loss:0.3291054666042328\n",
            "epoch:0 Batch: 1500 loss:0.055179618299007416\n",
            "epoch:0 Batch: 1600 loss:0.05321278050541878\n",
            "epoch:0 Batch: 1700 loss:0.07852675765752792\n",
            "epoch:0 Batch: 1800 loss:0.4744560718536377\n",
            "epoch:0 Batch: 1900 loss:0.03948891535401344\n",
            "epoch:0 Batch: 2000 loss:0.5566567182540894\n",
            "epoch:0 Batch: 2100 loss:0.026368603110313416\n",
            "epoch:0 Batch: 2200 loss:0.05482446029782295\n",
            "epoch:0 Batch: 2300 loss:0.04502648860216141\n",
            "epoch:0 Batch: 2400 loss:0.14238706231117249\n",
            "epoch:0 Batch: 2500 loss:0.09642040729522705\n",
            "epoch:0 Batch: 2600 loss:0.04012634977698326\n",
            "epoch:0 Batch: 2700 loss:0.018391495570540428\n",
            "epoch:0 Batch: 2800 loss:0.03186991065740585\n",
            "epoch:0 Batch: 2900 loss:0.10387355089187622\n",
            "epoch:0 Batch: 3000 loss:0.007758188061416149\n",
            "epoch:0 Batch: 3100 loss:0.010106547735631466\n",
            "epoch:0 Batch: 3200 loss:0.006876646541059017\n",
            "epoch:0 Batch: 3300 loss:0.007388265337795019\n",
            "epoch:0 Batch: 3400 loss:0.029875200241804123\n",
            "epoch:0 Batch: 3500 loss:0.004284966737031937\n",
            "epoch:0 Batch: 3600 loss:0.3836284875869751\n",
            "epoch:0 Batch: 3700 loss:0.009248415939509869\n",
            "epoch:0 Batch: 3800 loss:0.09157977253198624\n",
            "epoch:0 Batch: 3900 loss:0.037285976111888885\n",
            "epoch:0 Batch: 4000 loss:0.01960762031376362\n",
            "epoch:0 Batch: 4100 loss:0.0040822019800543785\n",
            "epoch:0 Batch: 4200 loss:0.0038223876617848873\n",
            "epoch:0 Batch: 4300 loss:0.0019595404155552387\n",
            "epoch:0 Batch: 4400 loss:0.2853398621082306\n",
            "epoch:0 Batch: 4500 loss:0.0008029189775697887\n",
            "epoch:0 Batch: 4600 loss:0.100554920732975\n",
            "epoch:0 Batch: 4700 loss:0.10335651785135269\n",
            "epoch:0 Batch: 4800 loss:0.0021286322735249996\n",
            "epoch:0 Batch: 4900 loss:0.27050668001174927\n",
            "epoch:0 Batch: 5000 loss:0.04208856448531151\n",
            "epoch:0 Batch: 5100 loss:0.031467415392398834\n",
            "epoch:0 Batch: 5200 loss:0.03366946429014206\n",
            "epoch:0 Batch: 5300 loss:0.05421092361211777\n",
            "epoch:0 Batch: 5400 loss:0.0569545142352581\n",
            "epoch:0 Batch: 5500 loss:0.0018098928267136216\n",
            "epoch:0 Batch: 5600 loss:0.0046558743342757225\n",
            "epoch:0 Batch: 5700 loss:0.19046862423419952\n",
            "epoch:0 Batch: 5800 loss:0.4495726227760315\n",
            "epoch:0 Batch: 5900 loss:0.020157651975750923\n",
            "epoch:0 Batch: 6000 loss:0.00038789428072050214\n",
            "epoch:1 Batch: 100 loss:0.16275431215763092\n",
            "epoch:1 Batch: 200 loss:0.013084744103252888\n",
            "epoch:1 Batch: 300 loss:0.00078145123552531\n",
            "epoch:1 Batch: 400 loss:0.07116180658340454\n",
            "epoch:1 Batch: 500 loss:0.00829227827489376\n",
            "epoch:1 Batch: 600 loss:0.005851339548826218\n",
            "epoch:1 Batch: 700 loss:0.00858617015182972\n",
            "epoch:1 Batch: 800 loss:0.049824029207229614\n",
            "epoch:1 Batch: 900 loss:0.002140082186087966\n",
            "epoch:1 Batch: 1000 loss:0.012146287597715855\n",
            "epoch:1 Batch: 1100 loss:0.033806078135967255\n",
            "epoch:1 Batch: 1200 loss:0.3855525553226471\n",
            "epoch:1 Batch: 1300 loss:0.12768013775348663\n",
            "epoch:1 Batch: 1400 loss:0.001670104218646884\n",
            "epoch:1 Batch: 1500 loss:0.017537157982587814\n",
            "epoch:1 Batch: 1600 loss:0.01697724498808384\n",
            "epoch:1 Batch: 1700 loss:0.003991781268268824\n",
            "epoch:1 Batch: 1800 loss:0.004819948226213455\n",
            "epoch:1 Batch: 1900 loss:0.004343960899859667\n",
            "epoch:1 Batch: 2000 loss:0.0019016513833776116\n",
            "epoch:1 Batch: 2100 loss:0.002873033983632922\n",
            "epoch:1 Batch: 2200 loss:0.008562169037759304\n",
            "epoch:1 Batch: 2300 loss:0.0011961974669247866\n",
            "epoch:1 Batch: 2400 loss:0.003216963727027178\n",
            "epoch:1 Batch: 2500 loss:0.0057013072073459625\n",
            "epoch:1 Batch: 2600 loss:0.0086183687672019\n",
            "epoch:1 Batch: 2700 loss:0.05109550431370735\n",
            "epoch:1 Batch: 2800 loss:0.011795401573181152\n",
            "epoch:1 Batch: 2900 loss:0.00198625260964036\n",
            "epoch:1 Batch: 3000 loss:0.0332382395863533\n",
            "epoch:1 Batch: 3100 loss:0.010257115587592125\n",
            "epoch:1 Batch: 3200 loss:0.025756221264600754\n",
            "epoch:1 Batch: 3300 loss:0.00703115900978446\n",
            "epoch:1 Batch: 3400 loss:0.00014211455709300935\n",
            "epoch:1 Batch: 3500 loss:0.0017011186573654413\n",
            "epoch:1 Batch: 3600 loss:0.5372857451438904\n",
            "epoch:1 Batch: 3700 loss:0.00380358356051147\n",
            "epoch:1 Batch: 3800 loss:0.016691969707608223\n",
            "epoch:1 Batch: 3900 loss:0.010013089515268803\n",
            "epoch:1 Batch: 4000 loss:0.05203569680452347\n",
            "epoch:1 Batch: 4100 loss:0.033697742968797684\n",
            "epoch:1 Batch: 4200 loss:0.04561494290828705\n",
            "epoch:1 Batch: 4300 loss:0.0024184961803257465\n",
            "epoch:1 Batch: 4400 loss:0.01462831161916256\n",
            "epoch:1 Batch: 4500 loss:0.11577753722667694\n",
            "epoch:1 Batch: 4600 loss:0.001072644954547286\n",
            "epoch:1 Batch: 4700 loss:7.250734779518098e-05\n",
            "epoch:1 Batch: 4800 loss:0.0007510822033509612\n",
            "epoch:1 Batch: 4900 loss:0.004627855494618416\n",
            "epoch:1 Batch: 5000 loss:0.008827468380331993\n",
            "epoch:1 Batch: 5100 loss:0.0015575947472825646\n",
            "epoch:1 Batch: 5200 loss:0.030782008543610573\n",
            "epoch:1 Batch: 5300 loss:0.0052227117121219635\n",
            "epoch:1 Batch: 5400 loss:0.0001173773780465126\n",
            "epoch:1 Batch: 5500 loss:0.0017221421003341675\n",
            "epoch:1 Batch: 5600 loss:0.02392694540321827\n",
            "epoch:1 Batch: 5700 loss:0.09406425058841705\n",
            "epoch:1 Batch: 5800 loss:0.2580351233482361\n",
            "epoch:1 Batch: 5900 loss:0.005629508290439844\n",
            "epoch:1 Batch: 6000 loss:0.14201366901397705\n",
            "epoch:2 Batch: 100 loss:0.004658979829400778\n",
            "epoch:2 Batch: 200 loss:0.0014678726438432932\n",
            "epoch:2 Batch: 300 loss:0.0018529373919591308\n",
            "epoch:2 Batch: 400 loss:0.0011747885728254914\n",
            "epoch:2 Batch: 500 loss:0.008383275009691715\n",
            "epoch:2 Batch: 600 loss:0.023733172565698624\n",
            "epoch:2 Batch: 700 loss:0.07152462750673294\n",
            "epoch:2 Batch: 800 loss:0.01065434142947197\n",
            "epoch:2 Batch: 900 loss:0.07162456214427948\n",
            "epoch:2 Batch: 1000 loss:0.0003675628686323762\n",
            "epoch:2 Batch: 1100 loss:0.004820610396564007\n",
            "epoch:2 Batch: 1200 loss:0.003455493599176407\n",
            "epoch:2 Batch: 1300 loss:0.003996501676738262\n",
            "epoch:2 Batch: 1400 loss:0.10128126293420792\n",
            "epoch:2 Batch: 1500 loss:0.03829234838485718\n",
            "epoch:2 Batch: 1600 loss:0.0001324451732216403\n",
            "epoch:2 Batch: 1700 loss:0.0069061219692230225\n",
            "epoch:2 Batch: 1800 loss:0.0008372392621822655\n",
            "epoch:2 Batch: 1900 loss:0.0012369012692943215\n",
            "epoch:2 Batch: 2000 loss:0.0032195281237363815\n",
            "epoch:2 Batch: 2100 loss:0.00033566690399311483\n",
            "epoch:2 Batch: 2200 loss:0.09020505100488663\n",
            "epoch:2 Batch: 2300 loss:0.003710924880579114\n",
            "epoch:2 Batch: 2400 loss:0.010705141350626945\n",
            "epoch:2 Batch: 2500 loss:0.002597618382424116\n",
            "epoch:2 Batch: 2600 loss:0.002448849845677614\n",
            "epoch:2 Batch: 2700 loss:0.0013181170215830207\n",
            "epoch:2 Batch: 2800 loss:0.0006063530454412103\n",
            "epoch:2 Batch: 2900 loss:0.051529139280319214\n",
            "epoch:2 Batch: 3000 loss:0.008078320883214474\n",
            "epoch:2 Batch: 3100 loss:8.54559984873049e-05\n",
            "epoch:2 Batch: 3200 loss:0.008801540359854698\n",
            "epoch:2 Batch: 3300 loss:0.002315933583304286\n",
            "epoch:2 Batch: 3400 loss:0.04247397184371948\n",
            "epoch:2 Batch: 3500 loss:0.4826599061489105\n",
            "epoch:2 Batch: 3600 loss:0.0011862406972795725\n",
            "epoch:2 Batch: 3700 loss:0.0004671173228416592\n",
            "epoch:2 Batch: 3800 loss:0.18910911679267883\n",
            "epoch:2 Batch: 3900 loss:0.8818497657775879\n",
            "epoch:2 Batch: 4000 loss:0.004668953828513622\n",
            "epoch:2 Batch: 4100 loss:0.5605572462081909\n",
            "epoch:2 Batch: 4200 loss:0.038080841302871704\n",
            "epoch:2 Batch: 4300 loss:0.008781859651207924\n",
            "epoch:2 Batch: 4400 loss:5.0956441555172205e-05\n",
            "epoch:2 Batch: 4500 loss:0.0013848241651430726\n",
            "epoch:2 Batch: 4600 loss:0.11475088447332382\n",
            "epoch:2 Batch: 4700 loss:0.01844201795756817\n",
            "epoch:2 Batch: 4800 loss:0.0016068397089838982\n",
            "epoch:2 Batch: 4900 loss:0.10292898118495941\n",
            "epoch:2 Batch: 5000 loss:0.0052436357364058495\n",
            "epoch:2 Batch: 5100 loss:0.0037413183599710464\n",
            "epoch:2 Batch: 5200 loss:0.00044493755558505654\n",
            "epoch:2 Batch: 5300 loss:0.001266677980311215\n",
            "epoch:2 Batch: 5400 loss:0.138673797249794\n",
            "epoch:2 Batch: 5500 loss:0.0012035723775625229\n",
            "epoch:2 Batch: 5600 loss:0.23892326653003693\n",
            "epoch:2 Batch: 5700 loss:0.00020772431162185967\n",
            "epoch:2 Batch: 5800 loss:0.005735010374337435\n",
            "epoch:2 Batch: 5900 loss:0.0020468076691031456\n",
            "epoch:2 Batch: 6000 loss:0.2449204921722412\n",
            "epoch:3 Batch: 100 loss:0.0034888293594121933\n",
            "epoch:3 Batch: 200 loss:2.8500955522758886e-05\n",
            "epoch:3 Batch: 300 loss:0.002246399177238345\n",
            "epoch:3 Batch: 400 loss:0.0006427995976991951\n",
            "epoch:3 Batch: 500 loss:8.810721919871867e-05\n",
            "epoch:3 Batch: 600 loss:0.007151054684072733\n",
            "epoch:3 Batch: 700 loss:0.0002747389080468565\n",
            "epoch:3 Batch: 800 loss:0.0005832043243572116\n",
            "epoch:3 Batch: 900 loss:0.002397860400378704\n",
            "epoch:3 Batch: 1000 loss:0.002420456614345312\n",
            "epoch:3 Batch: 1100 loss:0.010268156416714191\n",
            "epoch:3 Batch: 1200 loss:0.011097034439444542\n",
            "epoch:3 Batch: 1300 loss:0.0037272325716912746\n",
            "epoch:3 Batch: 1400 loss:1.8452683434588835e-05\n",
            "epoch:3 Batch: 1500 loss:0.001114755286835134\n",
            "epoch:3 Batch: 1600 loss:0.017252644523978233\n",
            "epoch:3 Batch: 1700 loss:0.0016567744314670563\n",
            "epoch:3 Batch: 1800 loss:0.0017998721450567245\n",
            "epoch:3 Batch: 1900 loss:0.02255944348871708\n",
            "epoch:3 Batch: 2000 loss:0.0001249374618055299\n",
            "epoch:3 Batch: 2100 loss:0.0002223577321274206\n",
            "epoch:3 Batch: 2200 loss:0.00031020952155813575\n",
            "epoch:3 Batch: 2300 loss:0.00010700251732487231\n",
            "epoch:3 Batch: 2400 loss:0.0001049584461725317\n",
            "epoch:3 Batch: 2500 loss:0.00010281025606673211\n",
            "epoch:3 Batch: 2600 loss:0.00023200619034469128\n",
            "epoch:3 Batch: 2700 loss:0.00020148204930592328\n",
            "epoch:3 Batch: 2800 loss:0.0005592297529801726\n",
            "epoch:3 Batch: 2900 loss:0.0077646211721003056\n",
            "epoch:3 Batch: 3000 loss:0.0031431831885129213\n",
            "epoch:3 Batch: 3100 loss:9.070417581824586e-05\n",
            "epoch:3 Batch: 3200 loss:0.0004449288244359195\n",
            "epoch:3 Batch: 3300 loss:0.00022374668333213776\n",
            "epoch:3 Batch: 3400 loss:0.041505638509988785\n",
            "epoch:3 Batch: 3500 loss:0.000307438982417807\n",
            "epoch:3 Batch: 3600 loss:0.003668801160529256\n",
            "epoch:3 Batch: 3700 loss:0.0007357405847869813\n",
            "epoch:3 Batch: 3800 loss:0.012234039604663849\n",
            "epoch:3 Batch: 3900 loss:0.005957003682851791\n",
            "epoch:3 Batch: 4000 loss:0.002649243688210845\n",
            "epoch:3 Batch: 4100 loss:0.0073783821426332\n",
            "epoch:3 Batch: 4200 loss:0.0037249946035444736\n",
            "epoch:3 Batch: 4300 loss:0.0012213571462780237\n",
            "epoch:3 Batch: 4400 loss:0.00029500023811124265\n",
            "epoch:3 Batch: 4500 loss:0.00185534474439919\n",
            "epoch:3 Batch: 4600 loss:0.3022889792919159\n",
            "epoch:3 Batch: 4700 loss:8.164254541043192e-05\n",
            "epoch:3 Batch: 4800 loss:0.00015864608576521277\n",
            "epoch:3 Batch: 4900 loss:0.0018862324068322778\n",
            "epoch:3 Batch: 5000 loss:0.0002891890180762857\n",
            "epoch:3 Batch: 5100 loss:0.008887315168976784\n",
            "epoch:3 Batch: 5200 loss:0.008174343034625053\n",
            "epoch:3 Batch: 5300 loss:4.6270128223113716e-05\n",
            "epoch:3 Batch: 5400 loss:0.0796482041478157\n",
            "epoch:3 Batch: 5500 loss:0.0020891004242002964\n",
            "epoch:3 Batch: 5600 loss:0.008022280409932137\n",
            "epoch:3 Batch: 5700 loss:0.009194494225084782\n",
            "epoch:3 Batch: 5800 loss:0.08063241839408875\n",
            "epoch:3 Batch: 5900 loss:0.004202110227197409\n",
            "epoch:3 Batch: 6000 loss:0.0808732658624649\n",
            "epoch:4 Batch: 100 loss:0.009892234578728676\n",
            "epoch:4 Batch: 200 loss:4.5581309677800164e-05\n",
            "epoch:4 Batch: 300 loss:0.0018635403830558062\n",
            "epoch:4 Batch: 400 loss:0.015170900151133537\n",
            "epoch:4 Batch: 500 loss:0.002050828654319048\n",
            "epoch:4 Batch: 600 loss:0.014099588617682457\n",
            "epoch:4 Batch: 700 loss:0.0046275295317173\n",
            "epoch:4 Batch: 800 loss:0.07656345516443253\n",
            "epoch:4 Batch: 900 loss:0.36809536814689636\n",
            "epoch:4 Batch: 1000 loss:0.0005264768842607737\n",
            "epoch:4 Batch: 1100 loss:0.00045433902414515615\n",
            "epoch:4 Batch: 1200 loss:0.0382874570786953\n",
            "epoch:4 Batch: 1300 loss:0.014749611727893353\n",
            "epoch:4 Batch: 1400 loss:0.001583868870511651\n",
            "epoch:4 Batch: 1500 loss:3.340022885822691e-05\n",
            "epoch:4 Batch: 1600 loss:0.04046183079481125\n",
            "epoch:4 Batch: 1700 loss:0.00045725697418674827\n",
            "epoch:4 Batch: 1800 loss:0.16302265226840973\n",
            "epoch:4 Batch: 1900 loss:0.0009776813676580787\n",
            "epoch:4 Batch: 2000 loss:0.013100449927151203\n",
            "epoch:4 Batch: 2100 loss:0.05156739428639412\n",
            "epoch:4 Batch: 2200 loss:2.024066270678304e-05\n",
            "epoch:4 Batch: 2300 loss:0.0046860226429998875\n",
            "epoch:4 Batch: 2400 loss:0.02186887338757515\n",
            "epoch:4 Batch: 2500 loss:0.12920889258384705\n",
            "epoch:4 Batch: 2600 loss:0.05890991538763046\n",
            "epoch:4 Batch: 2700 loss:0.0009390294435434043\n",
            "epoch:4 Batch: 2800 loss:5.316678198141744e-06\n",
            "epoch:4 Batch: 2900 loss:4.137045834795572e-05\n",
            "epoch:4 Batch: 3000 loss:0.0024396399967372417\n",
            "epoch:4 Batch: 3100 loss:0.00039582830504514277\n",
            "epoch:4 Batch: 3200 loss:9.05974957277067e-06\n",
            "epoch:4 Batch: 3300 loss:0.7199223637580872\n",
            "epoch:4 Batch: 3400 loss:0.025933701545000076\n",
            "epoch:4 Batch: 3500 loss:0.005651670973747969\n",
            "epoch:4 Batch: 3600 loss:0.0013979513896629214\n",
            "epoch:4 Batch: 3700 loss:0.15011849999427795\n",
            "epoch:4 Batch: 3800 loss:0.005596406292170286\n",
            "epoch:4 Batch: 3900 loss:0.0005499210092239082\n",
            "epoch:4 Batch: 4000 loss:0.3114553689956665\n",
            "epoch:4 Batch: 4100 loss:0.0007158601656556129\n",
            "epoch:4 Batch: 4200 loss:0.000989563181065023\n",
            "epoch:4 Batch: 4300 loss:0.0019343763124197721\n",
            "epoch:4 Batch: 4400 loss:0.2834780514240265\n",
            "epoch:4 Batch: 4500 loss:0.06296271085739136\n",
            "epoch:4 Batch: 4600 loss:0.009675281122326851\n",
            "epoch:4 Batch: 4700 loss:0.16157478094100952\n",
            "epoch:4 Batch: 4800 loss:0.010317974723875523\n",
            "epoch:4 Batch: 4900 loss:0.0005364165408536792\n",
            "epoch:4 Batch: 5000 loss:0.019827699288725853\n",
            "epoch:4 Batch: 5100 loss:0.09587147831916809\n",
            "epoch:4 Batch: 5200 loss:0.0011757358442991972\n",
            "epoch:4 Batch: 5300 loss:0.0015994030982255936\n",
            "epoch:4 Batch: 5400 loss:0.16506639122962952\n",
            "epoch:4 Batch: 5500 loss:0.022060604766011238\n",
            "epoch:4 Batch: 5600 loss:0.0022615627385675907\n",
            "epoch:4 Batch: 5700 loss:0.0002589357609394938\n",
            "epoch:4 Batch: 5800 loss:0.006970219314098358\n",
            "epoch:4 Batch: 5900 loss:0.0002578049898147583\n",
            "epoch:4 Batch: 6000 loss:0.0027098222635686398\n",
            "3.769818397363027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data[4141][0].reshape(28,28)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ok9oTvM9_aVf",
        "outputId": "da0ec36e-18f9-420e-fb50-af1838d9cf0e"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.2667, 0.5804, 0.8549, 0.6471, 0.5804, 0.4431, 0.0863,\n",
              "         0.0000, 0.3137, 0.5137, 0.0431, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.5725, 0.9294, 0.9882, 0.9882, 0.9882, 0.9922, 0.9882, 0.8314,\n",
              "         0.3333, 0.8824, 0.9922, 0.5569, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0235,\n",
              "         0.7765, 0.9922, 0.9882, 0.9137, 0.8039, 0.4941, 0.4980, 0.6314, 0.9490,\n",
              "         0.9882, 0.9882, 0.9804, 0.4314, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3569,\n",
              "         0.9882, 0.9529, 0.6000, 0.0275, 0.0000, 0.0000, 0.0000, 0.0000, 0.6000,\n",
              "         0.9882, 0.9882, 0.5490, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.9373,\n",
              "         0.9882, 0.4863, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2784, 0.9765,\n",
              "         0.9882, 0.9882, 0.0706, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0863, 0.9922,\n",
              "         0.9922, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3412, 0.8902, 0.9922,\n",
              "         0.9529, 0.1373, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0863, 0.9882,\n",
              "         0.9882, 0.0000, 0.0000, 0.0000, 0.0000, 0.1412, 0.8902, 0.9882, 0.9608,\n",
              "         0.3216, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0078, 0.6863,\n",
              "         0.9882, 0.4314, 0.0235, 0.0000, 0.0235, 0.8078, 0.9922, 0.9882, 0.6980,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3255,\n",
              "         0.9490, 0.9922, 0.5216, 0.0588, 0.7451, 0.9882, 0.9922, 0.6627, 0.1647,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.6549, 0.9922, 0.9373, 0.4980, 0.9882, 0.9882, 0.8549, 0.0431, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.4863, 0.9529, 0.9922, 0.9922, 0.9922, 0.3490, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.4353, 0.9882, 0.9882, 0.5373, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.7137, 0.9882, 0.9882, 0.7490, 0.2118, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.7451, 0.9882, 0.8863, 0.9882, 0.5176, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.7451, 0.9216, 0.2000, 0.9216, 0.9686, 0.2078, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.3333, 0.9098, 0.0000, 0.3490, 1.0000, 0.6588, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.3333, 0.9333, 0.1098, 0.0000, 0.6824, 0.7686, 0.0275,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.1686, 0.9490, 0.8314, 0.4980, 0.9608, 0.9882, 0.0824,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.3373, 0.9059, 0.9882, 0.9922, 0.9059, 0.0627,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.7490, 0.9255, 0.3843, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000],\n",
              "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "         0.0000]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(test_data[4141][0].reshape(28,28))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "U1nEKCL5xBoT",
        "outputId": "c1c5ae6c-3a41-438e-ea9f-e9df81b20ce8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7e42124b1c60>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  output = model(test_data[4141][0].reshape(1,1,28,28))"
      ],
      "metadata": {
        "id": "eIdL1CHazJYo"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output.argmax()"
      ],
      "metadata": {
        "id": "ay_dCwWQzmk5",
        "outputId": "d0c1e38e-43f7-4f3d-ce98-f66713c459aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(8)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4KfqTKrOzoU3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}